{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: hidden;\">\n",
    "  <h1 style=\"float: left;\">Envint Assessment</h1>\n",
    "  <img src=\"/Users/dhruvyadav/Desktop/Assesments/Envint-Assignment/image.png\" alt=\"image.png\" height=\"50\" width=\"50\" style=\"float: right;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "This project implements a web application that uses a Large Language Model (LLM) to extract key information from PDFs. It enables users to manage multiple projects, each with its own set of PDFs, and ensures that questions are answered solely based on the PDFs uploaded for the active project. The application supports creating, listing, and deleting projects, switching between projects without re-uploading PDFs, and providing accurate, context-aware responses. Built with Streamlit, FAISS, and Groq AI, it is optimized with caching mechanisms for performance and features a simple interface for usability.<br><br>\n",
    "The comeplete code resides in `app.py`. The code has seperate sections for each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Libraries used**\n",
    "- **Streamlit**: For UI\n",
    "- **Langchain**: For eveloping the application\n",
    "- **Groq**: For model inferencing (It provides lightening fast model inferencing)\n",
    "- **OpenAI**: For generating embeddings <i>(I could've used opensource embeddings from hugging face but I don't have enough storage in my laptop, also OpenAI embeddings are the safest bet)</i>\n",
    "- **OS**: For managing project directories \n",
    "- **FAISS**: Vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Core Features**\n",
    "\n",
    "1. **Environment Setup**\n",
    "   - Loads API keys for GROQ (`GROQ_API_KEY`) and OpenAI (`OPENAI_API_KEY`) from environment variables.\n",
    "   - Along with this, llm is also initialized here as well.\n",
    "\n",
    "2. **Project Management**\n",
    "   - Allows creation, deletion, and selection of projects.\n",
    "   - Projects have isolated directories (`projects/<project_name>`) to store PDFs and vector stores.\n",
    "   - Session state (`st.session_state`) maintains project-specific data.\n",
    "\n",
    "3. **PDF Handling**\n",
    "   - Upload PDFs into the selected project's directory.\n",
    "   - Process and load PDFs using `PyPDFDirectoryLoader`.\n",
    "   - Split documents into manageable chunks with `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "4. **Vector Store**\n",
    "   - Uses `FAISS` for vector storage and retrieval, backed by OpenAI embeddings.\n",
    "   - Supports reloading previously saved vector stores for faster access.\n",
    "   - Embeds split documents into vector space for retrieval.\n",
    "\n",
    "5. **Retrieval Augmented Generation (RAG)**\n",
    "   - Combines FAISS-based retrieval with a chat-based LLM model (`ChatGroq`) for answering user questions.\n",
    "   - Uses a custom `ChatPromptTemplate` to ensure responses are contextually accurate.\n",
    "   - Creates a `retrieval_chain` for answering user questions.\n",
    "\n",
    "6. **Streamlit UI**\n",
    "   - Sidebar for project management (create, delete, and switch projects).\n",
    "   - Main area for uploading PDFs, initializing the vector store, and asking questions.\n",
    "   - Dynamic updates to session state for seamless interactivity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Structure**\n",
    "\n",
    "1. **Environment Initialization**\n",
    "- **LLM Setup**: Initializes the Groq AI-based Large Language Model (LLM) using the `ChatGroq` API.\n",
    "- **Directory Structure**: Creates a `projects` directory to store project-specific PDFs and vector stores.\n",
    "- **Session State**: Utilizes `st.session_state` to manage project-specific data and ensure seamless transitions between projects.\n",
    "\n",
    "\n",
    "\n",
    "2. **Project Management**\n",
    "- **Project Creation**: Allows users to create new projects, each with its own isolated directory (`projects/<project_name>`).\n",
    "- **Project Deletion**: Enables deletion of projects, including all associated files and cached states.\n",
    "- **Project Switching**: Supports dynamic switching between projects without requiring re-upload of PDFs or reinitialization of vector stores.\n",
    "\n",
    "Key functions:\n",
    "- `create_project`: Initializes a new project with the required directory structure.\n",
    "- `delete_project`: Cleans up project files and session state for deleted projects.\n",
    "- `set_current_project`: Updates session state to reflect the active project.\n",
    "\n",
    "\n",
    "\n",
    "3. **Retrieval-Augmented Generation (RAG)**\n",
    "- **Document Handling**: \n",
    "  - PDF files are uploaded and processed using `PyPDFDirectoryLoader` to extract text content.\n",
    "  - Text is split into manageable chunks with `RecursiveCharacterTextSplitter` to optimize embedding creation.\n",
    "- **Vector Store**: \n",
    "  - Embeddings are generated using OpenAI's embedding models and stored in a FAISS vector store for efficient retrieval.\n",
    "  - Existing vector stores are reused if available, avoiding redundant computations.\n",
    "- **Retrieval Chain**:\n",
    "  - Combines FAISS-based chunk retrieval with Groq AI's LLM for context-aware question answering.\n",
    "  - Uses a custom `ChatPromptTemplate` to ensure answers are strictly based on the uploaded PDFs.\n",
    "\n",
    "\n",
    "\n",
    "4. **User Interface (UI)**\n",
    "- **Framework**: Built with **Streamlit** for an interactive and intuitive experience.\n",
    "- **Features**:\n",
    "  - **Sidebar**: Includes options for creating, listing, deleting, and switching projects.\n",
    "  - **Main Area**: Allows users to upload PDFs, initialize vector stores, and ask questions.\n",
    "  - **Caching**: Uses `@st.cache_resource` to optimize performance for repeated LLM initialization and vector store loading.\n",
    "- **Seamless Workflow**:\n",
    "  - Users can easily upload PDFs, initialize the retrieval system, and query project-specific information.\n",
    "  - All operations update dynamically based on session state, ensuring smooth navigation and interaction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Workflow**\n",
    "\n",
    "1. **Project Management**\n",
    "- User creates a new project from the sidebar.\n",
    "- The project directory is initialized, and state is updated in st.session_state.\n",
    "\n",
    "2. **PDF Upload**\n",
    "- PDFs are uploaded via the file uploader and saved in the project's pdfs directory.\n",
    "- Uploaded documents are parsed, split into chunks, and embedded into a vector store.\n",
    "\n",
    "3. **Question Answering**\n",
    "- When a user asks a question, the system retrieves the most relevant chunks from the vector store.\n",
    "- The LLM processes the retrieved context to generate a project-specific answer.\n",
    "\n",
    "4. **Project Switching**\n",
    "- Switching between projects dynamically loads the relevant vector store and documents without requiring re-upload.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Demo**\n",
    "\n",
    "Watch the demo of the app here: <a href=\"https://www.youtube.com/watch?v=mOlA6L104-M\">Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
